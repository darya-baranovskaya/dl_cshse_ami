{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2iNohN1G851"
   },
   "source": [
    "# Введение в pytorch\n",
    "\n",
    "**Разработчик: Алексей Озерин**",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aosokin/dl_cshse_ami/blob/master/2021-fall/homeworks_small/shw2/DL21-fall-shw2.ipynb)\n"

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alvOhaG_Y8w5"
   },
   "source": [
    "# Устанавливаем pytorch\n",
    "\n",
    "## Linux/Mac/Windows\n",
    "\n",
    "\n",
    "На оффсайте pytorch.org/get-started/ надо выбрать подходящую конфигурацию и установить пакеты pytorch (версия 1.9) и соответствующий torchvision.\n",
    "\n",
    "На своей машине бывает удобно устанавливать нужные версии python, pytorch и torchvision с помощью [conda](https://docs.conda.io/en/latest/miniconda.html) в [виртуальное окружение](https://docs.python.org/3/tutorial/venv.html) ([документация конды](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) про это)\n",
    "\n",
    "В Google Colab на осень 2021 уже установлены нужные версии pytorch и torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "!wget --quiet --show-progress \"https://raw.githubusercontent.com/aosokin/dl_cshse_ami/master/2021-fall/homeworks_small/shw2/util.py\"\n",
    "!wget --quiet --show-progress \"https://raw.githubusercontent.com/aosokin/dl_cshse_ami/master/2021-fall/homeworks_small/shw2/progress.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDDd8zvPY8w_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_ru21cQY8xK"
   },
   "outputs": [],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "print(\"X :\\n %s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", np.dot(x, x.T))\n",
    "print(\"mean over cols :\\n%s\" % (x.mean(axis=-1)))\n",
    "print(\"cumsum of cols :\\n%s\" % (np.cumsum(x, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NqOKnrFY8xQ"
   },
   "outputs": [],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "x = torch.from_numpy(x).type(torch.FloatTensor) #or torch.arange(0,16).view(4,4)\n",
    "\n",
    "print(\"X :\\n%s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", torch.matmul(x, x.transpose(1, 0)))\n",
    "print(\"mean over cols :\\n\", torch.mean(x, dim=-1))\n",
    "print(\"cumsum of cols :\\n\", torch.cumsum(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxJiOWdJY8xW"
   },
   "source": [
    "## NumPy vs Pytorch\n",
    "\n",
    "Numpy и Pytorch не требуют описания статического графа вычислений. \n",
    "\n",
    "Можно отлаживаться с помощью pdb или просто print.\n",
    "\n",
    "API несколько различается:\n",
    "\n",
    "```\n",
    "x.reshape([1,2,8]) -> x.view(1,2,8)\n",
    "x.sum(axis=-1) -> x.sum(dim=-1)\n",
    "x.astype('int64') -> x.type(torch.int64)\n",
    "```\n",
    "\n",
    "\n",
    "Легко конвертировать между собой:\n",
    "\n",
    "```\n",
    "torch.from_numpy(npx) -- вернет Tensor\n",
    "tt.numpy() -- вернет Numpy Array\n",
    "```\n",
    "\n",
    "Преобразовать тензор из одного числа в обычное питоновское число:\n",
    "```\n",
    "torch.tensor([1]).item() -> 1\n",
    "```\n",
    "\n",
    "\n",
    "Если что:\n",
    "- смотрите документацию https://pytorch.org/docs/\n",
    "- гуглите (Stackoverflow/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWKDglA_Y8xX"
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 2 * np.pi, 16, dtype=torch.float64)\n",
    "\n",
    "#Mini-task: compute a vector of sin^2(x) + cos^2(x)\n",
    "out = <your code here>\n",
    "\n",
    "print(out.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvBccGm8Y8xh"
   },
   "source": [
    "# Automatic gradients\n",
    "\n",
    "У каждого тензора в Pytorch есть флаг `requires_grad`, который отвечает за автоматическое вычисление градиентов:\n",
    "\n",
    "1. Создать переменную: `a = torch.tensor(..., requires_grad=True)`\n",
    "\n",
    "2. Определить какую-нибудь дифференцируемую функцию `loss = whatever(a)`\n",
    "\n",
    "3. Запросить обратный проход `loss.backward()`\n",
    "\n",
    "4. Градиенты будут доступны в `a.grads`\n",
    "\n",
    "\n",
    "Есть два важных отличия Pytorch от Theano/TF:\n",
    "\n",
    "1. Функцию ошибки можно изменять динамически, например на каждом минибатче.\n",
    "\n",
    "2. После вычисления `.backward()` градиенты сохраняются в `.grad` каждой задействованной переменной, при повторных вызовах градиенты суммируются. Это позволяет использовать несколько функций ошибок или виртуально увеличивать batch_size. Поэтому, после каждого шага оптимизатора градиенты стоит обнулять.\n",
    "\n",
    "\n",
    "\n",
    "## Leaf vs Non-leaf Variable:\n",
    "```\n",
    "x = torch.tensor([1., 2., 3., 4.], requires_grad=True))  # leaf tensor\n",
    "y = x + 1  # not a leaf variable\n",
    "```\n",
    "\n",
    "Градиенты будут сохранены и доступны для использования только для `leaf tensor`.\n",
    "Такое поведение по-умолчанию сделано ради экономии памяти. Все тензоры с флагом `requires_grad = False` считаются`leaf tensors` по умолчанию.\n",
    "\n",
    "\n",
    "Обратите внимание, что вычисление градиентов работает только для тензоров с вещественным типом данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCMFdtG1kbKo"
   },
   "outputs": [],
   "source": [
    "# will not work\n",
    "x = torch.tensor([1, 2, 3, 4], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zIjcCUJ1T1V"
   },
   "source": [
    "\n",
    "Чтобы выставить флаг `requires_grad=False` и выключить автоматическое вычисление градиентов для нескольких тензоров, можно использовать `with torch.no_grad()` или `detach`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JQjCVpb1Uy1"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "y = x**2\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.exp(x)\n",
    "    print(z.requires_grad)\n",
    "    \n",
    "# detach from the graph\n",
    "w = torch.log(x).detach()\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NHwzSqakbUt"
   },
   "source": [
    "Рассмотрим пример линейной регрессии на датасете Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sVwJ5DfY8xj"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "x, y = load_boston(return_X_y=True)\n",
    "\n",
    "#select one column for simplicity. \n",
    "x = x[:, -1] / x[:, -1].std()\n",
    "y = y / y.std()\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqXtxfdjY8xr"
   },
   "outputs": [],
   "source": [
    "# model tensors\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# data tensors\n",
    "x = torch.from_numpy(x).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# все тензоры являются leaf-tensors\n",
    "# x и y не требуют вычисления градиентов\n",
    "for vv in [w, b, x, y]:\n",
    "    print(vv.is_leaf, vv.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxTI1pQhY8x2"
   },
   "outputs": [],
   "source": [
    "#try out gradients\n",
    "y_pred = w * x + b\n",
    "loss = torch.mean((y_pred - y)**2)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "#now w.grad is a tensor containing gradient of L w.r.t. w\n",
    "\n",
    "print(\"dL/dw = \\n\", w.grad)\n",
    "print(\"dL/db = \\n\", b.grad)\n",
    "\n",
    "# no gradients for tensors with requires_grad=False\n",
    "# and non-leaf tensors\n",
    "print(\"Non-Leaf x dL/dx = \\n\", x.grad)\n",
    "print(\"Non-Leaf loss dL/dpred = \\n\", y_pred.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWC6fHzWY8x6"
   },
   "source": [
    "## Градиенты промежуточных вершин\n",
    "\n",
    "В графе, который мы описали `x` и `y_pread` не являются листовыми вершинами. По умолчанию для них не сохраняются градиенты.\n",
    "\n",
    "Для промежуточных вершин мы можем запросить сохранение градиентов с помощью функции `.retain_grad()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsPhBEdlY8x7"
   },
   "outputs": [],
   "source": [
    "y_pred = w * x + b\n",
    "\n",
    "# check this:\n",
    "y_pred.retain_grad()\n",
    "\n",
    "loss = torch.mean((y_pred - y)**2)\n",
    "loss.backward()\n",
    "\n",
    "print(\"Non-Leaf loss dL/dpred = \\n\", y_pred.grad[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qSGyePQY8x-"
   },
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mj_-PaVzY8yA"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    #compute loss\n",
    "    y_pred = w * x  + b\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "    \n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient descent step for weights\n",
    "    # take alpha about 0.1\n",
    "    <your code>\n",
    "    \n",
    "    #zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    #the rest of code is just bells and whistles\n",
    "    if (i + 1) % 5==0:\n",
    "        #draw linear regression prediction vs data\n",
    "        clear_output(True)\n",
    "        plt.axhline(0, color='gray')\n",
    "        plt.axvline(0, color='gray')\n",
    "        plt.scatter(x.numpy(),y.numpy())\n",
    "        plt.plot(x.numpy(),y_pred.data.numpy(),color='orange')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.item())\n",
    "        if loss.item() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiEVpQBQY8yE"
   },
   "source": [
    "# Optimizers\n",
    "\n",
    "В этом примере мы пользовались простым правилом для градиентного спуска:\n",
    "  \n",
    "$$\\theta^{n+1} = \\theta^{n} - \\alpha \\nabla_{\\theta}L$$\n",
    "\n",
    "\n",
    "Единственным параметром в нем является $\\alpha$ -- это `learning_rate`.\n",
    "\n",
    "На практике часто используют различные модификации (например _Momentum_):\n",
    "\n",
    "$$\\theta^{n+1} = \\theta^{n} - U^{n}\\\\\n",
    "U^{n} = \\gamma U^{n-1} + \\alpha \\nabla_{\\theta}(L)\n",
    "$$\n",
    "\n",
    "Хороший обзор алгоритмов оптимизации для сетей можно посмотреть [тут](http://ruder.io/optimizing-gradient-descent/).\n",
    "\n",
    "\n",
    "\n",
    "Pytorch предоставляет практически все широкораспространненные оптимизаторы:    \n",
    "http://pytorch.org/docs/master/optim.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Оптимизаторы удобны в использовании:\n",
    "\n",
    "- требуется указать список переменных для оптимизации\n",
    "- `opt.step()` применяет `update` ($U^{n}$) к весам\n",
    "- `opt.zero_grad()` сбрасывает градиенты\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crmZenu4Y8yF"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "x, y = load_boston(return_X_y=True)\n",
    "x = x[:, -1] / x[:, -1].std()\n",
    "y = y / y.std()\n",
    "\n",
    "# data tensors\n",
    "x = torch.from_numpy(x).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# model tensors\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# define optimizer\n",
    "opt = torch.optim.RMSprop([w, b], lr=0.1)\n",
    "\n",
    "for i in range(100):\n",
    "    # compute loss\n",
    "    loss = <your code>\n",
    "    \n",
    "    # backprop and gradient descent\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    #the rest of code is just bells and whistles\n",
    "    if (i + 1) % 5 == 0:\n",
    "        #draw linear regression prediction vs data\n",
    "        clear_output(True)\n",
    "        plt.axhline(0, color='gray')\n",
    "        plt.axvline(0, color='gray')\n",
    "        plt.scatter(x.numpy(), y.numpy())\n",
    "        plt.plot(x.numpy(), y_pred.data.numpy(), color='orange')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.item())\n",
    "        if loss.item() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qoOsLGuY8yK"
   },
   "source": [
    "## Highlevel-API \n",
    "\n",
    "При работе с нейронными сетями становится неудобно контролировать переменные с весами по-отдельности. Pytorch предоставляет высокоуровневый API для моделей http://pytorch.org/docs/master/nn.html#torch.nn.Module.\n",
    "\n",
    "\n",
    "Чтобы воспользоваться моделью необходимо отнаследоваться от torch.nn.Module, определить слои и описать `forward`, `backward` будет вычислен автоматически.\n",
    "\n",
    "\n",
    "Для демонстрации снова воспользуемся MNIST'ом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP-mINCsY8yL"
   },
   "outputs": [],
   "source": [
    "# MNIST again\n",
    "from util import load_mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6, 6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(\"Label: %i\" % y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28, 28]), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPDAygXbY8yQ"
   },
   "outputs": [],
   "source": [
    "# Higher-level API:\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size=40):\n",
    "        super(Net, self).__init__()\n",
    "        # here you construct weights for layers\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # here you describe usage of layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # check log_softmax signature\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    # backward function computes automaticaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSuPd8FrY8yU"
   },
   "outputs": [],
   "source": [
    "# model interface:\n",
    "model = Net()\n",
    "tt = torch.from_numpy(X_train[:10, :].astype(np.float32))\n",
    "output = model(tt)\n",
    "\n",
    "print('Model outputs: \\n', output)\n",
    "# TODO: получите вероятности из output c помощью функций из torch\n",
    "# hint: см документацию к log_softmax\n",
    "probs = <your code>\n",
    "print('Probs: \\n', probs)\n",
    "\n",
    "# TODO: получите предсказание из output c помощью функций из torch\n",
    "pred = <your code>\n",
    "print('Pred: \\n', pred.data.numpy())\n",
    "print('Truth: \\n', y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9v9SIlDY8yZ"
   },
   "source": [
    "Тренировка сети\n",
    "\n",
    "Для тренировки сети нам требуется \n",
    "- итератор по данным\n",
    "- функция тренировки (прогон по данным, вычисление и применение градиентов)\n",
    "- функция валидации (прогон по тестовым данным, вычисление метрик)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVfeDcF-Y8yc"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# функция для итераций по минибатчам, из первого семинара\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in tqdm(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS8wn1JzY8yg"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, batchsize=32):\n",
    "    loss_log = []\n",
    "    model.train()\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=batchsize, shuffle=True):\n",
    "        # data preparation\n",
    "        data = torch.from_numpy(x_batch.astype(np.float32))\n",
    "        target = torch.from_numpy(y_batch.astype(np.int64))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # make a step\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log\n",
    "\n",
    "\n",
    "# TODO: написать функцию для валидации по X_val, y_val\n",
    "# hint: optimizer не нужен\n",
    "def test(model):\n",
    "    loss_log = []\n",
    "    model.eval()\n",
    "    <your code>\n",
    "    \n",
    "    return loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl02nUhoY8yj"
   },
   "source": [
    "Для отслеживания процедуры тренировки построить график вида\n",
    "![img](./progress.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vbRTCJmY8yk"
   },
   "outputs": [],
   "source": [
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    plt.figure()\n",
    "    <your code here>\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWEroE1_Y8yo"
   },
   "outputs": [],
   "source": [
    "train_log = []\n",
    "val_log = []\n",
    "\n",
    "model = Net()\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "batchsize = 32\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss = train(model, opt, batchsize=batchsize)\n",
    "    train_log.extend(train_loss)\n",
    "    \n",
    "    val_loss = np.mean(test(model))\n",
    "        \n",
    "    # TODO: график train_loss vs train_steps с точками val_loss vs trained_steps\n",
    "    <your code here>\n",
    "    # use your plot_history()\n",
    "    \n",
    "    # hint: train_log and val_log may contain data with different shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gW6UzITY8yt"
   },
   "source": [
    "## Метрики\n",
    "\n",
    "Logloss -- величина, которую трудно интерпретировать. \n",
    "Для отслеживания тренировки и сравнения моделей удобнее наблюдать за интерпретируемыми метриками, например точностью (accuracy)\n",
    "\n",
    "Модифицируйте код train/test функций так, чтобы помимо ошибки подсчитывалась точность за батч:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa9VK7d3Y8yv"
   },
   "outputs": [],
   "source": [
    "# TODO: добавьте подсчет точности\n",
    "def train(model, optimizer, batchsize=32):\n",
    "    loss_log, acc_log = [], []\n",
    "    \n",
    "    <your code>\n",
    "    \n",
    "    return loss_log, acc_log\n",
    "\n",
    "\n",
    "# TODO: добавьте подсчет точности:\n",
    "def test(model):\n",
    "    loss_log, acc_log = [], []\n",
    "    \n",
    "    <your code>\n",
    "    \n",
    "    return loss_log, acc_log\n",
    "\n",
    "\n",
    "train_log, train_acc_log = [], []\n",
    "val_log, val_acc_log = [], []\n",
    "\n",
    "model = Net()\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "batchsize = 32\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    # train\n",
    "    <your code>\n",
    "    \n",
    "    # test\n",
    "    <your code>\n",
    "    \n",
    "    # store metrics\n",
    "    <your code>\n",
    "    \n",
    "    # plot all metrics (loss and acc for train/val)\n",
    "    <your code>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oikbV3efY8yz"
   },
   "source": [
    "## Fine Tuning\n",
    "Для многих прикладных задач не существует больших датасетов с хорошей разметкой. \n",
    "Поэтому распространенным приемом является тренировка на похожем, но большом датасете и доучивание сети на целевом.\n",
    "\n",
    "Такой прием называют **Transfer Learning** или **Finetuning**.\n",
    "\n",
    "В сверточных сетях для классификации выделяют две части:\n",
    "- тело сети -- это набор сверток и пулингов (convolutions and poolings)\n",
    "- голову -- это MLP (набор полносвязных слоев) после которых делается softmax и получаются вероятности разных классов.\n",
    "\n",
    "\n",
    "Вычислительно простым вариантом finetuning является переучивание головы сети.\n",
    "\n",
    "\n",
    "Нам потребуется [предобученная модель](http://pytorch.org/docs/master/torchvision/datasets.html#torchvision-datasets) и датасет для нашей задачи.\n",
    "\n",
    "Предлагется воспользоваться моделью для ImageNet и датасетом  https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "\n",
    "\n",
    "В датасете содержатся картинки двух классов (`ants` и `bees`) разных размеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a1J3RKxY8y0"
   },
   "outputs": [],
   "source": [
    "# На Windows придется скачать архив по ссылке (~45Mb) и распаковать самостоятельно\n",
    "!wget --quiet --show-progress \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
    "!unzip -q ./hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGVsiVQBY8y4"
   },
   "source": [
    "Загрузчик данных -- одна из важных компонент для эффективного обучения нейронных сетей:\n",
    "асинхронная загрузка и быстрая предобработка важны для полного использования GPU. В pytorch для этого есть https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "Пример использования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Emt8XDoY8y7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmUYoy7XY8zD"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.1)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5LNGH6DY8zK"
   },
   "outputs": [],
   "source": [
    "# обратите внимание на сохранение лучшей версии весов сети\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                if use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels).type(torch.float)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('Elapsed {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkG9Pa3mY8zP"
   },
   "outputs": [],
   "source": [
    "# torchvision содержит ряд моделей с претрейненными весами:\n",
    "[m for m in dir(models) if not m.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1ZvVhmnY8zS"
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "# hint: вы можете изучить устройство любого объекта в python пользуясь интерактивностью интерпретатора и методом dir()\n",
    "\n",
    "# Список слоев модели можно получить с помощью обхода\n",
    "# for x in model_ft.named_modules():\n",
    "#    print(x[0], x[1])\n",
    "\n",
    "# TODO: подмените в модели последний слой, чтобы она работала для двух классов\n",
    "\n",
    "<your code>\n",
    "\n",
    "# TODO: выберите, какие параметры дообучать. Результат получается лучше если дообучать всё или только последний слой? Почему?\n",
    "# например, выключить обучение всех параметров можно при помощи этого кода:\n",
    "# for params in model_ft.parameters():\n",
    "#     params.requires_grad = False \n",
    "\n",
    "params_to_train = <your code>\n",
    "\n",
    "# use GPU if you have it\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create optimizer on the selected parameters\n",
    "optimizer_ft = optim.SGD(params_to_train, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy1wlPmxY8zW"
   },
   "outputs": [],
   "source": [
    "model_ft = train_model(\n",
    "    model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n",
    "# если всё сделано правильно, то точность на валидации должна быть больше 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LR4Fi3qTY8zc"
   },
   "outputs": [],
   "source": [
    "# TODO: напишите функцию, прогоняющую модель на нескольких примерах из валидационной выборки\n",
    "# Отобразите картинки и предсказания\n",
    "\n",
    "def visualize(model, num_images=10):\n",
    "    <your code>\n",
    "            \n",
    "visualize(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfOyJKuaY8zi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hse_dl_year": "2021-fall"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
